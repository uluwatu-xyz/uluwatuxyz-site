<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Posts on uluwatu.xyz</title><link>https://www.uluwatu.xyz/posts/</link><description>Recent content in Posts on uluwatu.xyz</description><generator>Hugo</generator><language>en-us</language><copyright>Uluwatu.xyz 2024</copyright><lastBuildDate>Thu, 06 Feb 2025 12:13:03 +0000</lastBuildDate><atom:link href="https://www.uluwatu.xyz/posts/index.xml" rel="self" type="application/rss+xml"/><item><title>Debugging Python memory leaks with Heapy (Guppy)</title><link>https://www.uluwatu.xyz/posts/debugging_memleaks_w_heapy/</link><pubDate>Thu, 06 Feb 2025 12:13:03 +0000</pubDate><guid>https://www.uluwatu.xyz/posts/debugging_memleaks_w_heapy/</guid><description>&lt;h1 id="introduction">Introduction&lt;/h1>
&lt;p>Guppy &lt;sup id="fnref:1">&lt;a href="#fn:1" class="footnote-ref" role="doc-noteref">1&lt;/a>&lt;/sup> is a python library containing among other things the heap debugging tool Heapy. This came in useful recently when attempting to debug a memory leak in a long running Python process. Large enterprises are rumoured to restart server processes every weekend to work around such issues, but we prefer a more scientific solution.&lt;/p>
&lt;p>Unfortunately in the Python 3 port of Guppy the remote shell functionality for interactive debugging was disabled, although this would &amp;ldquo;stop the world&amp;rdquo; for other processing - not great for debugging production systems. To enable production profiling (as a last resort) we created a small utility class to write specific Guppy output to file on a schedule.&lt;/p></description></item><item><title>[draft] Signal Correlation Hurdle Rates For Profitable Trading</title><link>https://www.uluwatu.xyz/posts/ic-tcost-hurdle-rates/</link><pubDate>Mon, 30 Sep 2024 11:56:16 +0100</pubDate><guid>https://www.uluwatu.xyz/posts/ic-tcost-hurdle-rates/</guid><description>&lt;h1 id="introduction">Introduction&lt;/h1>
&lt;p>X (formally Twitter) user &lt;em>@macrocephalopod&lt;/em> &lt;sup id="fnref:1">&lt;a href="#fn:1" class="footnote-ref" role="doc-noteref">1&lt;/a>&lt;/sup> describes a derivation of the minimum required correlation between a trading signal and the target return for profitable trading. It is defined as a function of the signal Z-Score, forecast horizon return volatility and transaction costs.&lt;/p>
&lt;p>This blog post consists of a summary of this technique and applies it to historical Bitcoin price data.
We visualise the minimum correlation for different combinations of transaction costs (exchange fee tiers) and
target forecast horizons (via historical volatility).&lt;/p></description></item><item><title>Is the BTree an efficient structure to model an order book?</title><link>https://www.uluwatu.xyz/posts/btree_orderbook/</link><pubDate>Wed, 20 Dec 2023 13:29:35 +0000</pubDate><guid>https://www.uluwatu.xyz/posts/btree_orderbook/</guid><description>&lt;h1 id="introduction">Introduction&lt;/h1>
&lt;p>The order book is an interesting object to model, likely to be dense with orders near the top of the book and sparser as we walk outwards towards the less likely to get filled &amp;ldquo;stink bids&amp;rdquo;. While reading &lt;em>Database Internals: A Deep-Dive into How Distributed Data Systems Work&lt;/em> &lt;sup id="fnref:1">&lt;a href="#fn:1" class="footnote-ref" role="doc-noteref">1&lt;/a>&lt;/sup>, its low level discussion of BTree internals peaked my curiosity into the efficiency of using a BTree to model an orderbook. My main observations are:&lt;/p></description></item><item><title>Adjusted t-stat for overlapping samples</title><link>https://www.uluwatu.xyz/posts/adjusted_t-stat/</link><pubDate>Wed, 13 Dec 2023 19:56:41 +0000</pubDate><guid>https://www.uluwatu.xyz/posts/adjusted_t-stat/</guid><description>&lt;p>When computing a regression coefficient, we aim to reject the null hypothesis that there is no relationship between the dependent and independent variable ($ \beta_0 = 0 $). Similarly, when evaluating a trading strategies &lt;em>returns&lt;/em>, we aim to reject the null hypothesis that they are unprofitable random noise around 0.&lt;/p>
&lt;p>If we assume the distribution underlying the inputs are normal ($X_{0}$ for regression and $r_{t}$ for returns) we can use the &lt;em>t-statistic&lt;/em> to decide to reject these hypothesis&amp;rsquo; or not. The t-stat measures the number of standard errors (SE) the mean value is from a reference point ($ \beta_0 $). As discussed in our examples above our reference point is 0, so we can drop this term going forward. The SE is a estimator which uses the sample standard deviation ($\hat{\sigma}$) to estimate the population standard deviation ($\sigma$). Note that as the number of samples (N) increases the SE gets smaller, reflecting our increased confidence in our approximation of $\hat{\sigma}$ due to more datapoints - we&amp;rsquo;ll revisit this point later.&lt;/p></description></item></channel></rss>