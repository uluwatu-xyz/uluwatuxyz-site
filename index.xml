<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>uluwatu.xyz</title><link>https://www.uluwatu.xyz/</link><description>Recent content on uluwatu.xyz</description><generator>Hugo</generator><language>en-us</language><copyright>Uluwatu.xyz 2024</copyright><lastBuildDate>Mon, 30 Sep 2024 11:56:16 +0100</lastBuildDate><atom:link href="https://www.uluwatu.xyz/index.xml" rel="self" type="application/rss+xml"/><item><title>[draft] Signal Correlation Hurdle Rates For Profitable Trading</title><link>https://www.uluwatu.xyz/posts/ic-tcost-hurdle-rates/</link><pubDate>Mon, 30 Sep 2024 11:56:16 +0100</pubDate><guid>https://www.uluwatu.xyz/posts/ic-tcost-hurdle-rates/</guid><description>&lt;h1 id="introduction">Introduction&lt;/h1>
&lt;p>X (formally Twitter) user &lt;em>@macrocephalopod&lt;/em> &lt;sup id="fnref:1">&lt;a href="#fn:1" class="footnote-ref" role="doc-noteref">1&lt;/a>&lt;/sup> describes a derivation of the minimum required correlation between a trading signal and the target return for profitable trading. It is defined as a function of the signal Z-Score, forecast horizon return volatility and transaction costs.&lt;/p>
&lt;p>This blog post consists of a summary of this technique and applies it to historical Bitcoin price data.
We visualise the minimum correlation for different combinations of transaction costs (exchange fee tiers) and
target forecast horizons (via historical volatility).&lt;/p></description></item><item><title>Is the BTree an efficient structure to model an order book?</title><link>https://www.uluwatu.xyz/posts/btree_orderbook/</link><pubDate>Wed, 20 Dec 2023 13:29:35 +0000</pubDate><guid>https://www.uluwatu.xyz/posts/btree_orderbook/</guid><description>&lt;h1 id="introduction">Introduction&lt;/h1>
&lt;p>The order book is an interesting object to model, likely to be dense with orders near the top of the book and sparser as we walk outwards towards the less likely to get filled &amp;ldquo;stink bids&amp;rdquo;. While reading &lt;em>Database Internals: A Deep-Dive into How Distributed Data Systems Work&lt;/em> &lt;sup id="fnref:1">&lt;a href="#fn:1" class="footnote-ref" role="doc-noteref">1&lt;/a>&lt;/sup>, its low level discussion of BTree internals peaked my curiosity into the efficiency of using a BTree to model an orderbook. My main observations are:&lt;/p></description></item><item><title>Adjusted t-stat for overlapping samples</title><link>https://www.uluwatu.xyz/posts/adjusted_t-stat/</link><pubDate>Wed, 13 Dec 2023 19:56:41 +0000</pubDate><guid>https://www.uluwatu.xyz/posts/adjusted_t-stat/</guid><description>&lt;p>When computing a regression coefficient, we aim to reject the null hypothesis that there is no relationship between the dependent and independent variable ($ \beta_0 = 0 $). Similarly, when evaluating a trading strategies &lt;em>returns&lt;/em>, we aim to reject the null hypothesis that they are unprofitable random noise around 0.&lt;/p>
&lt;p>If we assume the distribution underlying the inputs are normal ($X_{0}$ for regression and $r_{t}$ for returns) we can use the &lt;em>t-statistic&lt;/em> to decide to reject these hypothesis&amp;rsquo; or not. The t-stat measures the number of standard errors (SE) the mean value is from a reference point ($ \beta_0 $). As discussed in our examples above our reference point is 0, so we can drop this term going forward. The SE is a estimator which uses the sample standard deviation ($\hat{\sigma}$) to estimate the population standard deviation ($\sigma$). Note that as the number of samples (N) increases the SE gets smaller, reflecting our increased confidence in our approximation of $\hat{\sigma}$ due to more datapoints - we&amp;rsquo;ll revisit this point later.&lt;/p></description></item></channel></rss>